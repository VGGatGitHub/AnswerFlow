{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "chatbot_v6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VGGatGitHub/AnswerFlow/blob/master/chatbot_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIF2hQ5wGsqg",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/datasets?search=nq-train\n",
        "\n",
        "V2: added comads to look at the structure of the train.json file and to assess the %s.\n",
        "\n",
        "V3: changing the code to do tarining for long_answesrs or short_answers using training_for_long_answer switch.\n",
        "\n",
        "V4: reading in json file produced from jsonl using jsonl2json.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xPDM8ocY74i",
        "colab_type": "code",
        "outputId": "dff3bcea-67e7-4e6c-beaa-8bddca5fb909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ed-1NnYIGsqj",
        "colab_type": "code",
        "outputId": "884de095-71a3-45b1-fe4c-c87eb9e19aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys \n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QByI39lkVt8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG define the foldre to inspect for files \n",
        "path='/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3ej-f3iYGsqm",
        "colab_type": "code",
        "outputId": "0878a82b-5ff8-4b1c-afe4-8b068e723973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#make sure the file you what is in the correct directory\n",
        "#some possible files are train.json or train200.json \n",
        "\n",
        "for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/chatbot_v2 (1).ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/train.json\n",
            "/content/drive/My Drive/Colab Notebooks/loding_files.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/Copy of qa.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/chatbot_v2.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/simplified-nq-train.jsonl\n",
            "/content/drive/My Drive/Colab Notebooks/chatbot_v3.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/text_utils.py\n",
            "/content/drive/My Drive/Colab Notebooks/jsonl2json.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/chatbot_v6.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/train200.json\n",
            "/content/drive/My Drive/Colab Notebooks/train200L.json\n",
            "/content/drive/My Drive/Colab Notebooks/Copy of jsonl2json.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/__pycache__/text_utils.cpython-36.pyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_S1byB1WPph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG read in training data\n",
        "#you may have to adjust the BATCH_SIZE acordingly \n",
        "filename='train200.json' #or train.json or train200L.json \n",
        "\n",
        "\n",
        "file_to_read=path+filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS2xGhZCoFxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG\n",
        "# you may need to get the file text_utils.py from \n",
        "# https://github.com/VGGatGitHub/natural-questions\n",
        "#\n",
        "\n",
        "sys.path.append(os.path.abspath(path))\n",
        "from text_utils import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EhFsBX5aKR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG The cell has been removed since now the data is analized in the jsonl2json.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pgXArJmIaiv",
        "colab_type": "code",
        "outputId": "280b86a1-0497-483b-e314-afb7d73ccd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7bPY5ic2eC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbW-oSFI2mFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG make sure the file_to_read has been defined above! \n",
        "\n",
        "UNKNOWN = \"<UNKNOWN>\"\n",
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset():\n",
        "    source = []\n",
        "    target = []\n",
        "    context = []\n",
        "\n",
        "    n_short_answers=0 #VGG\n",
        "    n_long=0\n",
        "    training_for_long_answer = False #True #False \n",
        "    \n",
        "    with open(file_to_read) as json_file: #VGG\n",
        "        data = json.load(json_file)\n",
        "\n",
        "        for nq_doc in data:\n",
        "            if filename == 'train200L.json':\n",
        "              doc = simplify_nq_example(nq_doc) #VGG for jsonl formated file\n",
        "            else:\n",
        "              doc=nq_doc\n",
        "\n",
        "            question_text = doc['question_text']\n",
        "            document_text = doc['document_text'].split()\n",
        "            long_answer_candidates = doc['long_answer_candidates']\n",
        "            annotations = doc['annotations'][0]\n",
        "            \n",
        "            if annotations['long_answer']['start_token'] < annotations['long_answer']['end_token']:\n",
        "                \n",
        "                n_long+=1\n",
        "                long_answer = \" \".join(document_text[annotations['long_answer']['start_token']:\n",
        "                                                     annotations['long_answer']['end_token']])\n",
        "                                      \n",
        "                if len(annotations['short_answers']) > 0:\n",
        "                    start_token = annotations['short_answers'][0]['start_token']\n",
        "                    end_token = annotations['short_answers'][0]['end_token']\n",
        "                    short_answer = \" \".join(document_text[start_token:end_token])\n",
        "                    n_short_answers+=1 #VGG\n",
        "                else:\n",
        "                    short_answer = UNKNOWN\n",
        "                \n",
        "                #VGG V3\n",
        "                if training_for_long_answer :\n",
        "                    short_answer=long_answer #VGG V3 change - make the target to be the long answer instead of the short answer \n",
        "                    for posibilities in long_answer_candidates:\n",
        "                        if posibilities[\"top_level\"]:\n",
        "                            start_token = posibilities['start_token']\n",
        "                            end_token = posibilities['end_token']                    \n",
        "                            posibility = \" \".join(document_text[start_token:end_token])\n",
        "                            context.append(preprocess_sentence(posibility))\n",
        "                else:\n",
        "                    context.append(preprocess_sentence(long_answer))\n",
        "                \n",
        "                #VGG context = [] #VGG it seems to work better!\n",
        "\n",
        "                source.append(preprocess_sentence(question_text))\n",
        "                target.append(preprocess_sentence(short_answer))\n",
        "#VGG                \n",
        "        print(\"Data set contains:\", n_short_answers,\"short answers out of\",\n",
        "              n_long,\"possible long answers, rate is {:.0f}%\".format(100*n_short_answers/n_long))    \n",
        "    return target, source, context\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4ecf3676-acd1-45e5-9459-3f9030b8b059",
        "_cell_guid": "3f83746e-4fa4-4e24-98cb-2f0df139a6af",
        "trusted": true,
        "id": "jevAjFFiGsqr",
        "colab_type": "code",
        "outputId": "08589c43-6ac8-4a0b-c6a7-d24c89066573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "    \n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset():\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang, context_lang = create_dataset()\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "    \n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "    \n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "# Try experimenting with the size of that dataset\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)    \n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 1 #VGG\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)    \n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "    \n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ = tf.multiply(loss_, mask)\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "\n",
        "#VGG uncommented for possible checkpoint saving later \n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)  \n",
        "\n",
        "                                 \n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss\n",
        "  \n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index.get(i, 0) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    result ='<start> '#VGG \n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot\n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set contians: 70 short answers out of 101 possible long answers, rate is 69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OMsxPEhOGsqt",
        "colab_type": "code",
        "outputId": "95340765-3d40-48b0-ebba-77094a3e4dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "#VGG for epoch in range(EPOCHS):\n",
        "epoch=-1\n",
        "total_loss=1\n",
        "total_loss_cut=0.01*steps_per_epoch\n",
        "\n",
        "print(\"\\nStarting training of at most {} epochs or until total loss is les than {:0.4f}\".format(EPOCHS,total_loss_cut))\n",
        "while (epoch < EPOCHS) and (total_loss > total_loss_cut):\n",
        "  epoch+=1\n",
        "\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch%8 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  '''\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  '''   \n",
        "\n",
        "  print('Epoch {} Total Loss {:.4f}'.format(epoch + 1, total_loss))\n",
        "  print('Time taken for this epoch {:.4f} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training of at most 100 epochs or until total loss is les than 0.8000\n",
            "Epoch 1 Batch 0 Loss 0.5445\n",
            "Epoch 1 Batch 8 Loss 0.8582\n",
            "Epoch 1 Batch 16 Loss 0.3186\n",
            "Epoch 1 Batch 24 Loss 0.2060\n",
            "Epoch 1 Batch 32 Loss 0.3134\n",
            "Epoch 1 Batch 40 Loss 0.5052\n",
            "Epoch 1 Batch 48 Loss 1.0866\n",
            "Epoch 1 Batch 56 Loss 2.9200\n",
            "Epoch 1 Batch 64 Loss 1.0876\n",
            "Epoch 1 Batch 72 Loss 0.5014\n",
            "Epoch 1 Total Loss 58.9683\n",
            "Time taken for this epoch 52.3484 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0608\n",
            "Epoch 2 Batch 8 Loss 2.2327\n",
            "Epoch 2 Batch 16 Loss 0.0154\n",
            "Epoch 2 Batch 24 Loss 0.0762\n",
            "Epoch 2 Batch 32 Loss 0.5553\n",
            "Epoch 2 Batch 40 Loss 0.4938\n",
            "Epoch 2 Batch 48 Loss 0.8827\n",
            "Epoch 2 Batch 56 Loss 0.0780\n",
            "Epoch 2 Batch 64 Loss 4.6570\n",
            "Epoch 2 Batch 72 Loss 2.6371\n",
            "Epoch 2 Total Loss 50.2365\n",
            "Time taken for this epoch 21.9901 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9752\n",
            "Epoch 3 Batch 8 Loss 0.0411\n",
            "Epoch 3 Batch 16 Loss 0.3643\n",
            "Epoch 3 Batch 24 Loss 0.0679\n",
            "Epoch 3 Batch 32 Loss 0.3481\n",
            "Epoch 3 Batch 40 Loss 0.6428\n",
            "Epoch 3 Batch 48 Loss 0.5898\n",
            "Epoch 3 Batch 56 Loss 0.0344\n",
            "Epoch 3 Batch 64 Loss 0.0308\n",
            "Epoch 3 Batch 72 Loss 0.2117\n",
            "Epoch 3 Total Loss 45.1502\n",
            "Time taken for this epoch 22.0473 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7716\n",
            "Epoch 4 Batch 8 Loss 0.1670\n",
            "Epoch 4 Batch 16 Loss 0.2533\n",
            "Epoch 4 Batch 24 Loss 0.6607\n",
            "Epoch 4 Batch 32 Loss 0.0250\n",
            "Epoch 4 Batch 40 Loss 0.0165\n",
            "Epoch 4 Batch 48 Loss 0.3261\n",
            "Epoch 4 Batch 56 Loss 0.1794\n",
            "Epoch 4 Batch 64 Loss 0.0444\n",
            "Epoch 4 Batch 72 Loss 0.0349\n",
            "Epoch 4 Total Loss 40.6869\n",
            "Time taken for this epoch 22.2694 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0328\n",
            "Epoch 5 Batch 8 Loss 0.0453\n",
            "Epoch 5 Batch 16 Loss 0.7119\n",
            "Epoch 5 Batch 24 Loss 0.6571\n",
            "Epoch 5 Batch 32 Loss 0.0529\n",
            "Epoch 5 Batch 40 Loss 0.1621\n",
            "Epoch 5 Batch 48 Loss 0.2414\n",
            "Epoch 5 Batch 56 Loss 0.3237\n",
            "Epoch 5 Batch 64 Loss 0.1861\n",
            "Epoch 5 Batch 72 Loss 0.0282\n",
            "Epoch 5 Total Loss 38.1024\n",
            "Time taken for this epoch 22.0685 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0175\n",
            "Epoch 6 Batch 8 Loss 0.2615\n",
            "Epoch 6 Batch 16 Loss 0.0528\n",
            "Epoch 6 Batch 24 Loss 0.0306\n",
            "Epoch 6 Batch 32 Loss 0.0204\n",
            "Epoch 6 Batch 40 Loss 0.1615\n",
            "Epoch 6 Batch 48 Loss 0.2396\n",
            "Epoch 6 Batch 56 Loss 0.4648\n",
            "Epoch 6 Batch 64 Loss 0.3310\n",
            "Epoch 6 Batch 72 Loss 2.2325\n",
            "Epoch 6 Total Loss 36.8700\n",
            "Time taken for this epoch 22.0406 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1210\n",
            "Epoch 7 Batch 8 Loss 0.1491\n",
            "Epoch 7 Batch 16 Loss 0.4392\n",
            "Epoch 7 Batch 24 Loss 0.0491\n",
            "Epoch 7 Batch 32 Loss 1.3944\n",
            "Epoch 7 Batch 40 Loss 0.3857\n",
            "Epoch 7 Batch 48 Loss 0.2478\n",
            "Epoch 7 Batch 56 Loss 0.0401\n",
            "Epoch 7 Batch 64 Loss 0.1624\n",
            "Epoch 7 Batch 72 Loss 0.0074\n",
            "Epoch 7 Total Loss 32.8743\n",
            "Time taken for this epoch 22.2962 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2606\n",
            "Epoch 8 Batch 8 Loss 0.0072\n",
            "Epoch 8 Batch 16 Loss 0.0130\n",
            "Epoch 8 Batch 24 Loss 0.1557\n",
            "Epoch 8 Batch 32 Loss 0.0158\n",
            "Epoch 8 Batch 40 Loss 0.3300\n",
            "Epoch 8 Batch 48 Loss 0.1601\n",
            "Epoch 8 Batch 56 Loss 0.1803\n",
            "Epoch 8 Batch 64 Loss 0.2211\n",
            "Epoch 8 Batch 72 Loss 0.4394\n",
            "Epoch 8 Total Loss 26.2181\n",
            "Time taken for this epoch 22.4757 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2030\n",
            "Epoch 9 Batch 8 Loss 0.1805\n",
            "Epoch 9 Batch 16 Loss 0.1556\n",
            "Epoch 9 Batch 24 Loss 0.0056\n",
            "Epoch 9 Batch 32 Loss 0.3213\n",
            "Epoch 9 Batch 40 Loss 0.9999\n",
            "Epoch 9 Batch 48 Loss 0.3203\n",
            "Epoch 9 Batch 56 Loss 0.2932\n",
            "Epoch 9 Batch 64 Loss 0.2823\n",
            "Epoch 9 Batch 72 Loss 0.2144\n",
            "Epoch 9 Total Loss 21.3165\n",
            "Time taken for this epoch 21.6888 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1190\n",
            "Epoch 10 Batch 8 Loss 0.2282\n",
            "Epoch 10 Batch 16 Loss 0.2592\n",
            "Epoch 10 Batch 24 Loss 0.0003\n",
            "Epoch 10 Batch 32 Loss 0.0138\n",
            "Epoch 10 Batch 40 Loss 0.0188\n",
            "Epoch 10 Batch 48 Loss 0.0038\n",
            "Epoch 10 Batch 56 Loss 0.7574\n",
            "Epoch 10 Batch 64 Loss 0.7766\n",
            "Epoch 10 Batch 72 Loss 0.1020\n",
            "Epoch 10 Total Loss 18.8046\n",
            "Time taken for this epoch 21.9052 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3045\n",
            "Epoch 11 Batch 8 Loss 0.0007\n",
            "Epoch 11 Batch 16 Loss 0.0012\n",
            "Epoch 11 Batch 24 Loss 0.1836\n",
            "Epoch 11 Batch 32 Loss 0.1545\n",
            "Epoch 11 Batch 40 Loss 0.2681\n",
            "Epoch 11 Batch 48 Loss 0.1321\n",
            "Epoch 11 Batch 56 Loss 0.0003\n",
            "Epoch 11 Batch 64 Loss 0.3544\n",
            "Epoch 11 Batch 72 Loss 1.7794\n",
            "Epoch 11 Total Loss 14.0269\n",
            "Time taken for this epoch 21.8433 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0034\n",
            "Epoch 12 Batch 8 Loss 0.1343\n",
            "Epoch 12 Batch 16 Loss 0.1555\n",
            "Epoch 12 Batch 24 Loss 0.0005\n",
            "Epoch 12 Batch 32 Loss 0.6846\n",
            "Epoch 12 Batch 40 Loss 0.2829\n",
            "Epoch 12 Batch 48 Loss 0.1041\n",
            "Epoch 12 Batch 56 Loss 0.1589\n",
            "Epoch 12 Batch 64 Loss 0.3660\n",
            "Epoch 12 Batch 72 Loss 0.1116\n",
            "Epoch 12 Total Loss 11.6853\n",
            "Time taken for this epoch 21.6868 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0923\n",
            "Epoch 13 Batch 8 Loss 0.1369\n",
            "Epoch 13 Batch 16 Loss 0.0054\n",
            "Epoch 13 Batch 24 Loss 0.1118\n",
            "Epoch 13 Batch 32 Loss 0.0003\n",
            "Epoch 13 Batch 40 Loss 0.0553\n",
            "Epoch 13 Batch 48 Loss 0.0034\n",
            "Epoch 13 Batch 56 Loss 0.1560\n",
            "Epoch 13 Batch 64 Loss 0.1132\n",
            "Epoch 13 Batch 72 Loss 0.0705\n",
            "Epoch 13 Total Loss 9.7937\n",
            "Time taken for this epoch 21.8103 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0670\n",
            "Epoch 14 Batch 8 Loss 0.0764\n",
            "Epoch 14 Batch 16 Loss 0.0002\n",
            "Epoch 14 Batch 24 Loss 0.1407\n",
            "Epoch 14 Batch 32 Loss 0.1168\n",
            "Epoch 14 Batch 40 Loss 0.1149\n",
            "Epoch 14 Batch 48 Loss 0.2633\n",
            "Epoch 14 Batch 56 Loss 0.1201\n",
            "Epoch 14 Batch 64 Loss 0.1499\n",
            "Epoch 14 Batch 72 Loss 0.1507\n",
            "Epoch 14 Total Loss 9.4830\n",
            "Time taken for this epoch 21.8522 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0824\n",
            "Epoch 15 Batch 8 Loss 0.1434\n",
            "Epoch 15 Batch 16 Loss 0.1480\n",
            "Epoch 15 Batch 24 Loss 0.0596\n",
            "Epoch 15 Batch 32 Loss 0.1435\n",
            "Epoch 15 Batch 40 Loss 0.0027\n",
            "Epoch 15 Batch 48 Loss 0.1490\n",
            "Epoch 15 Batch 56 Loss 0.0005\n",
            "Epoch 15 Batch 64 Loss 0.8680\n",
            "Epoch 15 Batch 72 Loss 0.0018\n",
            "Epoch 15 Total Loss 8.5476\n",
            "Time taken for this epoch 22.2093 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0010\n",
            "Epoch 16 Batch 8 Loss 0.0011\n",
            "Epoch 16 Batch 16 Loss 0.0068\n",
            "Epoch 16 Batch 24 Loss 0.1416\n",
            "Epoch 16 Batch 32 Loss 0.2022\n",
            "Epoch 16 Batch 40 Loss 0.1017\n",
            "Epoch 16 Batch 48 Loss 0.1012\n",
            "Epoch 16 Batch 56 Loss 0.0163\n",
            "Epoch 16 Batch 64 Loss 0.0628\n",
            "Epoch 16 Batch 72 Loss 0.0572\n",
            "Epoch 16 Total Loss 8.2551\n",
            "Time taken for this epoch 21.2610 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0483\n",
            "Epoch 17 Batch 8 Loss 0.1004\n",
            "Epoch 17 Batch 16 Loss 0.1054\n",
            "Epoch 17 Batch 24 Loss 0.0557\n",
            "Epoch 17 Batch 32 Loss 0.0000\n",
            "Epoch 17 Batch 40 Loss 0.0521\n",
            "Epoch 17 Batch 48 Loss 0.0094\n",
            "Epoch 17 Batch 56 Loss 0.0714\n",
            "Epoch 17 Batch 64 Loss 0.0557\n",
            "Epoch 17 Batch 72 Loss 0.0694\n",
            "Epoch 17 Total Loss 7.6002\n",
            "Time taken for this epoch 21.6737 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0012\n",
            "Epoch 18 Batch 8 Loss 0.0014\n",
            "Epoch 18 Batch 16 Loss 0.0557\n",
            "Epoch 18 Batch 24 Loss 0.0007\n",
            "Epoch 18 Batch 32 Loss 0.0435\n",
            "Epoch 18 Batch 40 Loss 0.0742\n",
            "Epoch 18 Batch 48 Loss 0.0381\n",
            "Epoch 18 Batch 56 Loss 0.0317\n",
            "Epoch 18 Batch 64 Loss 0.0004\n",
            "Epoch 18 Batch 72 Loss 0.0210\n",
            "Epoch 18 Total Loss 5.4786\n",
            "Time taken for this epoch 21.3631 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0126\n",
            "Epoch 19 Batch 8 Loss 0.0001\n",
            "Epoch 19 Batch 16 Loss 0.0000\n",
            "Epoch 19 Batch 24 Loss 0.0068\n",
            "Epoch 19 Batch 32 Loss 0.1494\n",
            "Epoch 19 Batch 40 Loss 0.0101\n",
            "Epoch 19 Batch 48 Loss 0.0050\n",
            "Epoch 19 Batch 56 Loss 0.2816\n",
            "Epoch 19 Batch 64 Loss 0.1169\n",
            "Epoch 19 Batch 72 Loss 0.1150\n",
            "Epoch 19 Total Loss 4.4441\n",
            "Time taken for this epoch 21.2006 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1081\n",
            "Epoch 20 Batch 8 Loss 0.0781\n",
            "Epoch 20 Batch 16 Loss 0.0547\n",
            "Epoch 20 Batch 24 Loss 0.2402\n",
            "Epoch 20 Batch 32 Loss 0.0001\n",
            "Epoch 20 Batch 40 Loss 0.0797\n",
            "Epoch 20 Batch 48 Loss 0.0000\n",
            "Epoch 20 Batch 56 Loss 0.0093\n",
            "Epoch 20 Batch 64 Loss 0.0010\n",
            "Epoch 20 Batch 72 Loss 0.0007\n",
            "Epoch 20 Total Loss 3.2813\n",
            "Time taken for this epoch 21.3799 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0008\n",
            "Epoch 21 Batch 8 Loss 0.0005\n",
            "Epoch 21 Batch 16 Loss 0.0452\n",
            "Epoch 21 Batch 24 Loss 0.0006\n",
            "Epoch 21 Batch 32 Loss 0.0790\n",
            "Epoch 21 Batch 40 Loss 0.0002\n",
            "Epoch 21 Batch 48 Loss 0.0863\n",
            "Epoch 21 Batch 56 Loss 0.0000\n",
            "Epoch 21 Batch 64 Loss 0.0150\n",
            "Epoch 21 Batch 72 Loss 0.0012\n",
            "Epoch 21 Total Loss 3.1448\n",
            "Time taken for this epoch 21.2947 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0005\n",
            "Epoch 22 Batch 8 Loss 0.0000\n",
            "Epoch 22 Batch 16 Loss 0.0016\n",
            "Epoch 22 Batch 24 Loss 0.0922\n",
            "Epoch 22 Batch 32 Loss 0.1387\n",
            "Epoch 22 Batch 40 Loss 0.0017\n",
            "Epoch 22 Batch 48 Loss 0.0016\n",
            "Epoch 22 Batch 56 Loss 0.0001\n",
            "Epoch 22 Batch 64 Loss 0.0001\n",
            "Epoch 22 Batch 72 Loss 0.0000\n",
            "Epoch 22 Total Loss 3.1371\n",
            "Time taken for this epoch 21.6076 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0024\n",
            "Epoch 23 Batch 8 Loss 0.0000\n",
            "Epoch 23 Batch 16 Loss 0.6962\n",
            "Epoch 23 Batch 24 Loss 0.0058\n",
            "Epoch 23 Batch 32 Loss 0.0001\n",
            "Epoch 23 Batch 40 Loss 0.0131\n",
            "Epoch 23 Batch 48 Loss 0.0001\n",
            "Epoch 23 Batch 56 Loss 0.0107\n",
            "Epoch 23 Batch 64 Loss 0.0054\n",
            "Epoch 23 Batch 72 Loss 0.0000\n",
            "Epoch 23 Total Loss 2.7871\n",
            "Time taken for this epoch 23.0135 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.2993\n",
            "Epoch 24 Batch 8 Loss 0.0000\n",
            "Epoch 24 Batch 16 Loss 0.0235\n",
            "Epoch 24 Batch 24 Loss 0.0007\n",
            "Epoch 24 Batch 32 Loss 0.0000\n",
            "Epoch 24 Batch 40 Loss 0.0023\n",
            "Epoch 24 Batch 48 Loss 0.0005\n",
            "Epoch 24 Batch 56 Loss 0.0000\n",
            "Epoch 24 Batch 64 Loss 0.6503\n",
            "Epoch 24 Batch 72 Loss 0.0012\n",
            "Epoch 24 Total Loss 2.7622\n",
            "Time taken for this epoch 21.6834 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0027\n",
            "Epoch 25 Batch 8 Loss 0.0008\n",
            "Epoch 25 Batch 16 Loss 0.0002\n",
            "Epoch 25 Batch 24 Loss 0.0003\n",
            "Epoch 25 Batch 32 Loss 0.0067\n",
            "Epoch 25 Batch 40 Loss 0.0017\n",
            "Epoch 25 Batch 48 Loss 0.0133\n",
            "Epoch 25 Batch 56 Loss 0.0005\n",
            "Epoch 25 Batch 64 Loss 0.0006\n",
            "Epoch 25 Batch 72 Loss 0.0047\n",
            "Epoch 25 Total Loss 2.8119\n",
            "Time taken for this epoch 21.5868 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0013\n",
            "Epoch 26 Batch 8 Loss 0.0003\n",
            "Epoch 26 Batch 16 Loss 0.0013\n",
            "Epoch 26 Batch 24 Loss 0.0000\n",
            "Epoch 26 Batch 32 Loss 0.0011\n",
            "Epoch 26 Batch 40 Loss 0.0015\n",
            "Epoch 26 Batch 48 Loss 0.0024\n",
            "Epoch 26 Batch 56 Loss 0.0000\n",
            "Epoch 26 Batch 64 Loss 0.0000\n",
            "Epoch 26 Batch 72 Loss 0.0024\n",
            "Epoch 26 Total Loss 2.3903\n",
            "Time taken for this epoch 22.5786 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0892\n",
            "Epoch 27 Batch 8 Loss 0.0018\n",
            "Epoch 27 Batch 16 Loss 0.0007\n",
            "Epoch 27 Batch 24 Loss 0.0017\n",
            "Epoch 27 Batch 32 Loss 0.0005\n",
            "Epoch 27 Batch 40 Loss 0.0163\n",
            "Epoch 27 Batch 48 Loss 0.0000\n",
            "Epoch 27 Batch 56 Loss 0.0000\n",
            "Epoch 27 Batch 64 Loss 0.0031\n",
            "Epoch 27 Batch 72 Loss 0.0007\n",
            "Epoch 27 Total Loss 3.9010\n",
            "Time taken for this epoch 21.8791 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0000\n",
            "Epoch 28 Batch 8 Loss 0.0525\n",
            "Epoch 28 Batch 16 Loss 0.0043\n",
            "Epoch 28 Batch 24 Loss 0.0204\n",
            "Epoch 28 Batch 32 Loss 0.0000\n",
            "Epoch 28 Batch 40 Loss 0.0000\n",
            "Epoch 28 Batch 48 Loss 0.0139\n",
            "Epoch 28 Batch 56 Loss 0.0004\n",
            "Epoch 28 Batch 64 Loss 0.0228\n",
            "Epoch 28 Batch 72 Loss 0.0401\n",
            "Epoch 28 Total Loss 5.1845\n",
            "Time taken for this epoch 22.6612 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.2379\n",
            "Epoch 29 Batch 8 Loss 0.1737\n",
            "Epoch 29 Batch 16 Loss 0.0144\n",
            "Epoch 29 Batch 24 Loss 0.0240\n",
            "Epoch 29 Batch 32 Loss 0.0083\n",
            "Epoch 29 Batch 40 Loss 0.0013\n",
            "Epoch 29 Batch 48 Loss 0.0523\n",
            "Epoch 29 Batch 56 Loss 0.0237\n",
            "Epoch 29 Batch 64 Loss 0.0079\n",
            "Epoch 29 Batch 72 Loss 0.0108\n",
            "Epoch 29 Total Loss 11.1054\n",
            "Time taken for this epoch 21.9144 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0050\n",
            "Epoch 30 Batch 8 Loss 0.1813\n",
            "Epoch 30 Batch 16 Loss 0.0000\n",
            "Epoch 30 Batch 24 Loss 0.0022\n",
            "Epoch 30 Batch 32 Loss 0.0002\n",
            "Epoch 30 Batch 40 Loss 0.0267\n",
            "Epoch 30 Batch 48 Loss 0.0000\n",
            "Epoch 30 Batch 56 Loss 0.0001\n",
            "Epoch 30 Batch 64 Loss 0.8993\n",
            "Epoch 30 Batch 72 Loss 0.0091\n",
            "Epoch 30 Total Loss 6.9217\n",
            "Time taken for this epoch 21.8604 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0035\n",
            "Epoch 31 Batch 8 Loss 0.4332\n",
            "Epoch 31 Batch 16 Loss 0.0000\n",
            "Epoch 31 Batch 24 Loss 0.0066\n",
            "Epoch 31 Batch 32 Loss 0.0066\n",
            "Epoch 31 Batch 40 Loss 0.4802\n",
            "Epoch 31 Batch 48 Loss 0.0001\n",
            "Epoch 31 Batch 56 Loss 0.0107\n",
            "Epoch 31 Batch 64 Loss 0.0007\n",
            "Epoch 31 Batch 72 Loss 0.0051\n",
            "Epoch 31 Total Loss 4.9095\n",
            "Time taken for this epoch 22.1423 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.1683\n",
            "Epoch 32 Batch 8 Loss 0.0003\n",
            "Epoch 32 Batch 16 Loss 0.0003\n",
            "Epoch 32 Batch 24 Loss 0.0006\n",
            "Epoch 32 Batch 32 Loss 0.0000\n",
            "Epoch 32 Batch 40 Loss 0.0001\n",
            "Epoch 32 Batch 48 Loss 0.0022\n",
            "Epoch 32 Batch 56 Loss 0.0004\n",
            "Epoch 32 Batch 64 Loss 0.0000\n",
            "Epoch 32 Batch 72 Loss 0.0042\n",
            "Epoch 32 Total Loss 3.4640\n",
            "Time taken for this epoch 21.7371 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0004\n",
            "Epoch 33 Batch 8 Loss 0.0037\n",
            "Epoch 33 Batch 16 Loss 0.0029\n",
            "Epoch 33 Batch 24 Loss 0.0001\n",
            "Epoch 33 Batch 32 Loss 0.1273\n",
            "Epoch 33 Batch 40 Loss 0.0032\n",
            "Epoch 33 Batch 48 Loss 0.0000\n",
            "Epoch 33 Batch 56 Loss 0.0017\n",
            "Epoch 33 Batch 64 Loss 0.0543\n",
            "Epoch 33 Batch 72 Loss 0.2998\n",
            "Epoch 33 Total Loss 2.1692\n",
            "Time taken for this epoch 22.0947 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.1482\n",
            "Epoch 34 Batch 8 Loss 0.0004\n",
            "Epoch 34 Batch 16 Loss 0.0099\n",
            "Epoch 34 Batch 24 Loss 0.0014\n",
            "Epoch 34 Batch 32 Loss 0.0009\n",
            "Epoch 34 Batch 40 Loss 0.0000\n",
            "Epoch 34 Batch 48 Loss 0.0001\n",
            "Epoch 34 Batch 56 Loss 0.0018\n",
            "Epoch 34 Batch 64 Loss 0.0047\n",
            "Epoch 34 Batch 72 Loss 0.0000\n",
            "Epoch 34 Total Loss 2.0282\n",
            "Time taken for this epoch 22.2567 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0010\n",
            "Epoch 35 Batch 8 Loss 0.0000\n",
            "Epoch 35 Batch 16 Loss 0.0041\n",
            "Epoch 35 Batch 24 Loss 0.0004\n",
            "Epoch 35 Batch 32 Loss 0.0000\n",
            "Epoch 35 Batch 40 Loss 0.0004\n",
            "Epoch 35 Batch 48 Loss 0.0011\n",
            "Epoch 35 Batch 56 Loss 0.0008\n",
            "Epoch 35 Batch 64 Loss 0.2805\n",
            "Epoch 35 Batch 72 Loss 0.0002\n",
            "Epoch 35 Total Loss 1.8554\n",
            "Time taken for this epoch 22.1116 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0013\n",
            "Epoch 36 Batch 8 Loss 0.0003\n",
            "Epoch 36 Batch 16 Loss 0.0002\n",
            "Epoch 36 Batch 24 Loss 0.0009\n",
            "Epoch 36 Batch 32 Loss 0.0000\n",
            "Epoch 36 Batch 40 Loss 0.0003\n",
            "Epoch 36 Batch 48 Loss 0.0005\n",
            "Epoch 36 Batch 56 Loss 0.0590\n",
            "Epoch 36 Batch 64 Loss 0.0000\n",
            "Epoch 36 Batch 72 Loss 0.0000\n",
            "Epoch 36 Total Loss 1.2810\n",
            "Time taken for this epoch 21.9563 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0002\n",
            "Epoch 37 Batch 8 Loss 0.0000\n",
            "Epoch 37 Batch 16 Loss 0.0017\n",
            "Epoch 37 Batch 24 Loss 0.0007\n",
            "Epoch 37 Batch 32 Loss 0.0016\n",
            "Epoch 37 Batch 40 Loss 0.0008\n",
            "Epoch 37 Batch 48 Loss 0.0000\n",
            "Epoch 37 Batch 56 Loss 0.0003\n",
            "Epoch 37 Batch 64 Loss 0.0001\n",
            "Epoch 37 Batch 72 Loss 0.0000\n",
            "Epoch 37 Total Loss 1.2396\n",
            "Time taken for this epoch 23.1284 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.1257\n",
            "Epoch 38 Batch 8 Loss 0.0225\n",
            "Epoch 38 Batch 16 Loss 0.0005\n",
            "Epoch 38 Batch 24 Loss 0.0000\n",
            "Epoch 38 Batch 32 Loss 0.0001\n",
            "Epoch 38 Batch 40 Loss 0.0003\n",
            "Epoch 38 Batch 48 Loss 0.0000\n",
            "Epoch 38 Batch 56 Loss 0.0000\n",
            "Epoch 38 Batch 64 Loss 0.0000\n",
            "Epoch 38 Batch 72 Loss 0.0003\n",
            "Epoch 38 Total Loss 1.4423\n",
            "Time taken for this epoch 21.9629 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0002\n",
            "Epoch 39 Batch 8 Loss 0.0000\n",
            "Epoch 39 Batch 16 Loss 0.0000\n",
            "Epoch 39 Batch 24 Loss 0.0002\n",
            "Epoch 39 Batch 32 Loss 0.0001\n",
            "Epoch 39 Batch 40 Loss 0.0000\n",
            "Epoch 39 Batch 48 Loss 0.0000\n",
            "Epoch 39 Batch 56 Loss 0.0004\n",
            "Epoch 39 Batch 64 Loss 0.0024\n",
            "Epoch 39 Batch 72 Loss 0.0000\n",
            "Epoch 39 Total Loss 1.0423\n",
            "Time taken for this epoch 22.3014 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0003\n",
            "Epoch 40 Batch 8 Loss 0.0009\n",
            "Epoch 40 Batch 16 Loss 0.0002\n",
            "Epoch 40 Batch 24 Loss 0.2928\n",
            "Epoch 40 Batch 32 Loss 0.0000\n",
            "Epoch 40 Batch 40 Loss 0.0003\n",
            "Epoch 40 Batch 48 Loss 0.0000\n",
            "Epoch 40 Batch 56 Loss 0.0000\n",
            "Epoch 40 Batch 64 Loss 0.0000\n",
            "Epoch 40 Batch 72 Loss 0.0000\n",
            "Epoch 40 Total Loss 1.0207\n",
            "Time taken for this epoch 22.0767 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.2501\n",
            "Epoch 41 Batch 8 Loss 0.0000\n",
            "Epoch 41 Batch 16 Loss 0.0001\n",
            "Epoch 41 Batch 24 Loss 0.0005\n",
            "Epoch 41 Batch 32 Loss 0.0041\n",
            "Epoch 41 Batch 40 Loss 0.0002\n",
            "Epoch 41 Batch 48 Loss 0.0006\n",
            "Epoch 41 Batch 56 Loss 0.0002\n",
            "Epoch 41 Batch 64 Loss 0.0009\n",
            "Epoch 41 Batch 72 Loss 0.0009\n",
            "Epoch 41 Total Loss 1.0720\n",
            "Time taken for this epoch 21.7310 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0000\n",
            "Epoch 42 Batch 8 Loss 0.0000\n",
            "Epoch 42 Batch 16 Loss 0.0000\n",
            "Epoch 42 Batch 24 Loss 0.0000\n",
            "Epoch 42 Batch 32 Loss 0.0000\n",
            "Epoch 42 Batch 40 Loss 0.0000\n",
            "Epoch 42 Batch 48 Loss 0.0001\n",
            "Epoch 42 Batch 56 Loss 0.0053\n",
            "Epoch 42 Batch 64 Loss 0.0000\n",
            "Epoch 42 Batch 72 Loss 0.0004\n",
            "Epoch 42 Total Loss 1.1190\n",
            "Time taken for this epoch 22.0362 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0004\n",
            "Epoch 43 Batch 8 Loss 0.0022\n",
            "Epoch 43 Batch 16 Loss 0.0014\n",
            "Epoch 43 Batch 24 Loss 0.0003\n",
            "Epoch 43 Batch 32 Loss 0.0002\n",
            "Epoch 43 Batch 40 Loss 0.0003\n",
            "Epoch 43 Batch 48 Loss 0.0003\n",
            "Epoch 43 Batch 56 Loss 0.0000\n",
            "Epoch 43 Batch 64 Loss 0.0002\n",
            "Epoch 43 Batch 72 Loss 0.0000\n",
            "Epoch 43 Total Loss 1.0028\n",
            "Time taken for this epoch 22.1837 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0003\n",
            "Epoch 44 Batch 8 Loss 0.0002\n",
            "Epoch 44 Batch 16 Loss 0.0001\n",
            "Epoch 44 Batch 24 Loss 0.0000\n",
            "Epoch 44 Batch 32 Loss 0.0000\n",
            "Epoch 44 Batch 40 Loss 0.0004\n",
            "Epoch 44 Batch 48 Loss 0.0002\n",
            "Epoch 44 Batch 56 Loss 0.0004\n",
            "Epoch 44 Batch 64 Loss 0.2181\n",
            "Epoch 44 Batch 72 Loss 0.0790\n",
            "Epoch 44 Total Loss 1.2972\n",
            "Time taken for this epoch 22.3791 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0003\n",
            "Epoch 45 Batch 8 Loss 0.0000\n",
            "Epoch 45 Batch 16 Loss 0.0000\n",
            "Epoch 45 Batch 24 Loss 0.0003\n",
            "Epoch 45 Batch 32 Loss 0.0000\n",
            "Epoch 45 Batch 40 Loss 0.0898\n",
            "Epoch 45 Batch 48 Loss 0.0001\n",
            "Epoch 45 Batch 56 Loss 0.0008\n",
            "Epoch 45 Batch 64 Loss 0.0021\n",
            "Epoch 45 Batch 72 Loss 0.0001\n",
            "Epoch 45 Total Loss 1.4944\n",
            "Time taken for this epoch 22.2994 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0001\n",
            "Epoch 46 Batch 8 Loss 0.0009\n",
            "Epoch 46 Batch 16 Loss 0.0003\n",
            "Epoch 46 Batch 24 Loss 0.0023\n",
            "Epoch 46 Batch 32 Loss 0.1448\n",
            "Epoch 46 Batch 40 Loss 0.0011\n",
            "Epoch 46 Batch 48 Loss 0.0000\n",
            "Epoch 46 Batch 56 Loss 0.0013\n",
            "Epoch 46 Batch 64 Loss 0.0008\n",
            "Epoch 46 Batch 72 Loss 0.0017\n",
            "Epoch 46 Total Loss 2.2000\n",
            "Time taken for this epoch 22.3412 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0004\n",
            "Epoch 47 Batch 8 Loss 0.0003\n",
            "Epoch 47 Batch 16 Loss 0.0053\n",
            "Epoch 47 Batch 24 Loss 0.0198\n",
            "Epoch 47 Batch 32 Loss 0.2927\n",
            "Epoch 47 Batch 40 Loss 0.0002\n",
            "Epoch 47 Batch 48 Loss 0.0229\n",
            "Epoch 47 Batch 56 Loss 0.0000\n",
            "Epoch 47 Batch 64 Loss 0.0006\n",
            "Epoch 47 Batch 72 Loss 0.8585\n",
            "Epoch 47 Total Loss 3.4795\n",
            "Time taken for this epoch 22.7736 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0128\n",
            "Epoch 48 Batch 8 Loss 0.0358\n",
            "Epoch 48 Batch 16 Loss 0.0019\n",
            "Epoch 48 Batch 24 Loss 0.2196\n",
            "Epoch 48 Batch 32 Loss 0.3444\n",
            "Epoch 48 Batch 40 Loss 0.0003\n",
            "Epoch 48 Batch 48 Loss 0.0011\n",
            "Epoch 48 Batch 56 Loss 0.0002\n",
            "Epoch 48 Batch 64 Loss 0.0025\n",
            "Epoch 48 Batch 72 Loss 0.0011\n",
            "Epoch 48 Total Loss 3.7990\n",
            "Time taken for this epoch 22.2942 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.4228\n",
            "Epoch 49 Batch 8 Loss 0.0001\n",
            "Epoch 49 Batch 16 Loss 0.0002\n",
            "Epoch 49 Batch 24 Loss 0.2998\n",
            "Epoch 49 Batch 32 Loss 0.0000\n",
            "Epoch 49 Batch 40 Loss 0.0081\n",
            "Epoch 49 Batch 48 Loss 0.0001\n",
            "Epoch 49 Batch 56 Loss 0.0007\n",
            "Epoch 49 Batch 64 Loss 0.0033\n",
            "Epoch 49 Batch 72 Loss 0.2655\n",
            "Epoch 49 Total Loss 3.3644\n",
            "Time taken for this epoch 22.1480 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0336\n",
            "Epoch 50 Batch 8 Loss 0.1424\n",
            "Epoch 50 Batch 16 Loss 0.0002\n",
            "Epoch 50 Batch 24 Loss 0.0336\n",
            "Epoch 50 Batch 32 Loss 0.0005\n",
            "Epoch 50 Batch 40 Loss 0.1770\n",
            "Epoch 50 Batch 48 Loss 0.0012\n",
            "Epoch 50 Batch 56 Loss 0.0752\n",
            "Epoch 50 Batch 64 Loss 0.4062\n",
            "Epoch 50 Batch 72 Loss 0.0021\n",
            "Epoch 50 Total Loss 3.7129\n",
            "Time taken for this epoch 23.2333 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0011\n",
            "Epoch 51 Batch 8 Loss 0.0000\n",
            "Epoch 51 Batch 16 Loss 0.0000\n",
            "Epoch 51 Batch 24 Loss 0.0009\n",
            "Epoch 51 Batch 32 Loss 0.0207\n",
            "Epoch 51 Batch 40 Loss 0.0006\n",
            "Epoch 51 Batch 48 Loss 0.0020\n",
            "Epoch 51 Batch 56 Loss 0.0001\n",
            "Epoch 51 Batch 64 Loss 0.0001\n",
            "Epoch 51 Batch 72 Loss 0.0010\n",
            "Epoch 51 Total Loss 3.4673\n",
            "Time taken for this epoch 22.0857 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0031\n",
            "Epoch 52 Batch 8 Loss 0.0000\n",
            "Epoch 52 Batch 16 Loss 0.0002\n",
            "Epoch 52 Batch 24 Loss 0.2025\n",
            "Epoch 52 Batch 32 Loss 0.0052\n",
            "Epoch 52 Batch 40 Loss 0.0001\n",
            "Epoch 52 Batch 48 Loss 0.0808\n",
            "Epoch 52 Batch 56 Loss 0.0000\n",
            "Epoch 52 Batch 64 Loss 0.0052\n",
            "Epoch 52 Batch 72 Loss 0.0002\n",
            "Epoch 52 Total Loss 2.4884\n",
            "Time taken for this epoch 21.4480 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0000\n",
            "Epoch 53 Batch 8 Loss 0.0000\n",
            "Epoch 53 Batch 16 Loss 0.0000\n",
            "Epoch 53 Batch 24 Loss 0.0002\n",
            "Epoch 53 Batch 32 Loss 0.0494\n",
            "Epoch 53 Batch 40 Loss 0.0968\n",
            "Epoch 53 Batch 48 Loss 0.0647\n",
            "Epoch 53 Batch 56 Loss 0.0000\n",
            "Epoch 53 Batch 64 Loss 0.0001\n",
            "Epoch 53 Batch 72 Loss 0.0000\n",
            "Epoch 53 Total Loss 1.6455\n",
            "Time taken for this epoch 21.5175 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0000\n",
            "Epoch 54 Batch 8 Loss 0.0082\n",
            "Epoch 54 Batch 16 Loss 0.0008\n",
            "Epoch 54 Batch 24 Loss 0.0001\n",
            "Epoch 54 Batch 32 Loss 0.0070\n",
            "Epoch 54 Batch 40 Loss 0.0014\n",
            "Epoch 54 Batch 48 Loss 0.0001\n",
            "Epoch 54 Batch 56 Loss 0.0004\n",
            "Epoch 54 Batch 64 Loss 0.0009\n",
            "Epoch 54 Batch 72 Loss 0.0096\n",
            "Epoch 54 Total Loss 1.4427\n",
            "Time taken for this epoch 21.2955 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0000\n",
            "Epoch 55 Batch 8 Loss 0.0001\n",
            "Epoch 55 Batch 16 Loss 0.0003\n",
            "Epoch 55 Batch 24 Loss 0.3412\n",
            "Epoch 55 Batch 32 Loss 0.0001\n",
            "Epoch 55 Batch 40 Loss 0.0002\n",
            "Epoch 55 Batch 48 Loss 0.0000\n",
            "Epoch 55 Batch 56 Loss 0.0000\n",
            "Epoch 55 Batch 64 Loss 0.0035\n",
            "Epoch 55 Batch 72 Loss 0.0014\n",
            "Epoch 55 Total Loss 1.4270\n",
            "Time taken for this epoch 21.5916 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0001\n",
            "Epoch 56 Batch 8 Loss 0.0000\n",
            "Epoch 56 Batch 16 Loss 0.0002\n",
            "Epoch 56 Batch 24 Loss 0.0044\n",
            "Epoch 56 Batch 32 Loss 0.0003\n",
            "Epoch 56 Batch 40 Loss 0.0000\n",
            "Epoch 56 Batch 48 Loss 0.0005\n",
            "Epoch 56 Batch 56 Loss 0.0012\n",
            "Epoch 56 Batch 64 Loss 0.0000\n",
            "Epoch 56 Batch 72 Loss 0.0001\n",
            "Epoch 56 Total Loss 0.9649\n",
            "Time taken for this epoch 21.4583 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0000\n",
            "Epoch 57 Batch 8 Loss 0.0000\n",
            "Epoch 57 Batch 16 Loss 0.0001\n",
            "Epoch 57 Batch 24 Loss 0.0000\n",
            "Epoch 57 Batch 32 Loss 0.0001\n",
            "Epoch 57 Batch 40 Loss 0.0000\n",
            "Epoch 57 Batch 48 Loss 0.0000\n",
            "Epoch 57 Batch 56 Loss 0.0003\n",
            "Epoch 57 Batch 64 Loss 0.0002\n",
            "Epoch 57 Batch 72 Loss 0.0000\n",
            "Epoch 57 Total Loss 0.7146\n",
            "Time taken for this epoch 21.3480 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iI09zf-bGsqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "\n",
        "def ask(sentence):\n",
        "    result, sentence1, attention_plot = evaluate(sentence)\n",
        "    print('\\nQuestion: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))\n",
        "    return result\n",
        "\n",
        "def show_attention_plot(sentence):\n",
        "  result, sentence1, attention_plot = evaluate(sentence)\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "def is_it_known(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    if result.split() != ['<start>', 'unknown', '<end>']: return True\n",
        "    return False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7a7fe15c-ca68-49f0-bec9-059d7ef47e6a",
        "_cell_guid": "4684e38d-ab84-492d-8e2f-d13b1a0924fd",
        "trusted": true,
        "id": "4hRJh7EFGsqw",
        "colab_type": "code",
        "outputId": "c775a8d8-24b8-40e9-ffd3-3bd6607d368e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "ask('which is the most common use of opt-in e-mail marketing')    \n",
        "ask('most common use of opt-in e-mail marketing')\n",
        "ask('how did I meet your mother')\n",
        "ask('who is your mother')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Question: which is the most common use of opt-in e-mail marketing\n",
            "Predicted answer: <start> on the southern region of the land of texas , nebraska <end> \n",
            "\n",
            "Question: most common use of opt-in e-mail marketing\n",
            "Predicted answer: <start> yokota air base , western tokyo <end> \n",
            "\n",
            "Question: how did I meet your mother\n",
            "Predicted answer: <start> the supreme governor <end> \n",
            "\n",
            "Question: who is your mother\n",
            "Predicted answer: <start> chris pine <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> chris pine <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkzhBscq3zWi",
        "colab_type": "code",
        "outputId": "ae284615-9955-433f-8d8d-d2304f12552e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "source": [
        "show_attention_plot(\"which is the most common use of opt in e mail marketing\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAKACAYAAAC2QoMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VbL6X2bzGsq3",
        "colab_type": "code",
        "outputId": "60ab167e-5db8-4527-8bbe-329229fde7ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "show_attention_plot(\"who plays young flo in the progressive commercials\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAAHPCAYAAAArhxxpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iRhZwdJrWPG",
        "colab_type": "code",
        "outputId": "cbe15f3c-8ecc-4270-f7a6-8be5c9451af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "target, source, context = create_dataset()\n",
        "\n",
        "print(\"\\ngoing over all the questions and selecting those with answers ... \\n\")\n",
        "n_answers=0\n",
        "i=-1\n",
        "n_correct=0\n",
        "for question_text in source:\n",
        "    i+=1\n",
        "    if is_it_known(question_text):\n",
        "        n_answers+=1\n",
        "        AFanswer=ask(question_text)\n",
        "        if AFanswer.split() == target[i].split(): \n",
        "          n_correct+=1\n",
        "        else:\n",
        "          print(\"The answer was:\",target[i])\n",
        "          #print(question_text)\n",
        "          #print(AFanswer.split())\n",
        "          #print(target[i].split())\n",
        "          print('\\n')\n",
        "\n",
        "print(\"\\n{} answers out of {} possible, rate is {:.0f}%\".format(n_answers,len(source),100*n_answers/len(source)))\n",
        "print(\"At least {} correct answers out of {} possible, rate is {:.0f}%\\n\".format(n_correct,n_answers,100*n_correct/n_answers))\n",
        "           "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set contians: 70 short answers out of 101 possible long answers, rate is 69%\n",
            "\n",
            "going over all the questions and selecting those with answers ... \n",
            "\n",
            "\n",
            "Question: <start> when is the last episode of season of the walking dead <end>\n",
            "Predicted answer: <start> march , <end> \n",
            "\n",
            "Question: <start> in greek mythology who was the goddess of spring growth <end>\n",
            "Predicted answer: <start> persephone p r s f ni greek , also called kore k ri the maiden <end> \n",
            "\n",
            "Question: <start> what is the name of the most important jewish text <end>\n",
            "Predicted answer: <start> the shulchan aruch <end> \n",
            "\n",
            "Question: <start> what is the name of spain s most famous soccer team <end>\n",
            "Predicted answer: <start> real madrid <end> \n",
            "\n",
            "Question: <start> when was the first robot used in surgery <end>\n",
            "Predicted answer: <start> the supreme governor <end> \n",
            "The answer was: <start>  <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who sings the song i don t care i love it <end>\n",
            "Predicted answer: <start> icona pop and charli xcx <end> \n",
            "\n",
            "Question: <start> who are uncle owen and aunt beru related to <end>\n",
            "Predicted answer: <start> on the southern region of the land of canaan <end> \n",
            "The answer was: <start> shmi <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> where is zimbabwe located in the world map <end>\n",
            "Predicted answer: <start> in southern africa , botswana , bordered by south africa , botswana , bordered by south africa , botswana , bordered by south africa , botswana , bordered by south \n",
            "The answer was: <start> in southern africa , between the zambezi and limpopo rivers , bordered by south africa , botswana , zambia and mozambique <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> acres is equal to how many hectares <end>\n",
            "Predicted answer: <start> september , <end> \n",
            "The answer was: <start> square hectometre hm <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> where was donovan mitchell picked in the draft <end>\n",
            "Predicted answer: <start> th <end> \n",
            "\n",
            "Question: <start> where did the beatles final live performance take place <end>\n",
            "Predicted answer: <start> the roof of the band s multimedia corporation apple corps at savile row <end> \n",
            "The answer was: <start> the roof of the headquarters of the band s multimedia corporation apple corps at savile row <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who is the president of costa rica <end>\n",
            "Predicted answer: <start> luis guillermo solis rivera <end> \n",
            "\n",
            "Question: <start> when did power rangers tv show come out <end>\n",
            "Predicted answer: <start> august , <end> \n",
            "\n",
            "Question: <start> inheritance of the a b o blood groups is an example of <end>\n",
            "Predicted answer: <start> the zambezi and missouri . francis preparatory school <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> how many beverly hills cops movies are there <end>\n",
            "Predicted answer: <start> three <end> \n",
            "\n",
            "Question: <start> who holds the most women s wimbledon titles <end>\n",
            "Predicted answer: <start> cochran <end> \n",
            "The answer was: <start> martina navratilova <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> where does jinx you owe me a coke come from <end>\n",
            "Predicted answer: <start> jinx is a children s game with varying rules and penalties that occur when two people unintentionally speak the same word or phrase simultaneously <end> \n",
            "\n",
            "Question: <start> where does the last name hickey come from <end>\n",
            "Predicted answer: <start> irish origin <end> \n",
            "\n",
            "Question: <start> where is the greatest royal rumble taking place <end>\n",
            "Predicted answer: <start> jeddah , saudi arabia <end> \n",
            "\n",
            "Question: <start> who sang my name is tallulah in bugsy malone <end>\n",
            "Predicted answer: <start> louise liberty williams <end> \n",
            "\n",
            "Question: <start> what is andy s sisters name in toy story <end>\n",
            "Predicted answer: <start> cochran <end> \n",
            "The answer was: <start> molly <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who sings with shaggy on it wasn me <end>\n",
            "Predicted answer: <start> the band s game with varying rules and missouri . <end> \n",
            "The answer was: <start> english jamaican singer rikrok <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> what is the largest catholic high school in america <end>\n",
            "Predicted answer: <start> st . francis preparatory school <end> \n",
            "\n",
            "Question: <start> who established the peoples republic of china in <end>\n",
            "Predicted answer: <start> mao zedong <end> \n",
            "\n",
            "Question: <start> who sang take that look off your face <end>\n",
            "Predicted answer: <start> marti webb <end> \n",
            "\n",
            "Question: <start> who plays percy in the lost city of z <end>\n",
            "Predicted answer: <start> charlie hunnam <end> \n",
            "\n",
            "Question: <start> who plays the first lady on house of cards <end>\n",
            "Predicted answer: <start> joanna c . going <end> \n",
            "\n",
            "Question: <start> who owns st andrews golf course in scotland <end>\n",
            "Predicted answer: <start> the st andrews links trust <end> \n",
            "\n",
            "Question: <start> which city and state hosts the annual college world series <end>\n",
            "Predicted answer: <start> omaha , nebraska <end> \n",
            "\n",
            "Question: <start> who is ted talking about in how i met your mother <end>\n",
            "Predicted answer: <start> tracy mcconnell <end> \n",
            "\n",
            "Question: <start> when does grey s anatomy season premiere <end>\n",
            "Predicted answer: <start> september , <end> \n",
            "\n",
            "Question: <start> the partial pressure of carbon dioxide in arterial blood is approximately <end>\n",
            "Predicted answer: <start> the zambezi and mozambique <end> \n",
            "The answer was: <start> at sea level mmhg in arterial blood is between mmhg and mmhg <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> what was uncle jesse s original last name on full house <end>\n",
            "Predicted answer: <start> cochran <end> \n",
            "\n",
            "Question: <start> who pays the judgements on the judge mathis show <end>\n",
            "Predicted answer: <start> greg mathis <end> \n",
            "\n",
            "Question: <start> who made up the elf on the shelf <end>\n",
            "Predicted answer: <start> st . francis preparatory school <end> \n",
            "The answer was: <start> chanda bell <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who warned concord that the british were coming <end>\n",
            "Predicted answer: <start> samuel prescott <end> \n",
            "\n",
            "Question: <start> who lasted the longest on alone season <end>\n",
            "Predicted answer: <start> david mcintyre <end> \n",
            "\n",
            "Question: <start> where is the us military base in japan <end>\n",
            "Predicted answer: <start> yokota air base , fussa , western tokyo <end> \n",
            "\n",
            "Question: <start> who determines the size of the supreme court <end>\n",
            "Predicted answer: <start> congress <end> \n",
            "\n",
            "Question: <start> who is the actor that plays lucifer on tv <end>\n",
            "Predicted answer: <start> thomas john ellis <end> \n",
            "\n",
            "Question: <start> who designed the national coat of arms of south africa <end>\n",
            "Predicted answer: <start> the zambezi and missouri . francis preparatory school <end> \n",
            "The answer was: <start> iaan bekker <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> when does model code of conduct come into force <end>\n",
            "Predicted answer: <start> immediately on announcement of the election schedule by the commission <end> \n",
            "\n",
            "Question: <start> when did the dolan twins post their first youtube video <end>\n",
            "Predicted answer: <start> november <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> when did now thats what i call music come out <end>\n",
            "Predicted answer: <start> november <end> \n",
            "\n",
            "Question: <start> where is the grey cup being played <end>\n",
            "Predicted answer: <start> the brick field at commonwealth stadium in edmonton , alberta <end> \n",
            "\n",
            "Question: <start> where did the band bastille get their name <end>\n",
            "Predicted answer: <start> bastille day <end> \n",
            "\n",
            "Question: <start> where is arkansas river located on a map <end>\n",
            "Predicted answer: <start> the arkansas river flows through colorado , oklahoma , and arkansas , new mexico and missouri . <end> \n",
            "The answer was: <start> the arkansas river flows through colorado , kansas , oklahoma , and arkansas , and its watershed also drains parts of texas , new mexico and missouri . <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who plays nicholas in the princess diaries <end>\n",
            "Predicted answer: <start> chris pine <end> \n",
            "\n",
            "Question: <start> the p wave phase of an electrocardiogram ecg represents <end>\n",
            "Predicted answer: <start> atrial depolarization <end> \n",
            "\n",
            "Question: <start> when did star trek the next generation first air <end>\n",
            "Predicted answer: <start> september , <end> \n",
            "\n",
            "Question: <start> why was the civil rights act of made into an amendment to the constitution <end>\n",
            "Predicted answer: <start> cochran <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who sang the most wonderful summer of my life <end>\n",
            "Predicted answer: <start> louise liberty williams <end> \n",
            "The answer was: <start> jackie ward <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> how many oscars did on golden pond win <end>\n",
            "Predicted answer: <start> three <end> \n",
            "\n",
            "Question: <start> who won season of america s got talent <end>\n",
            "Predicted answer: <start> kevin skinner <end> \n",
            "\n",
            "Question: <start> how many episodes on queen of the south season <end>\n",
            "Predicted answer: <start> <end> \n",
            "\n",
            "Question: <start> who is the existing prime minister of pakistan <end>\n",
            "Predicted answer: <start> imran khan <end> \n",
            "\n",
            "Question: <start> who is known as the father of texas <end>\n",
            "Predicted answer: <start> stephen fuller austin <end> \n",
            "\n",
            "Question: <start> properties of red black tree in data structure <end>\n",
            "Predicted answer: <start> the zambezi and mozambique <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> what is the orange stuff on my sushi <end>\n",
            "Predicted answer: <start> the supreme governor <end> \n",
            "The answer was: <start> tobiko <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> when did nsw last won a state of origin series <end>\n",
            "Predicted answer: <start> <end> \n",
            "\n",
            "Question: <start> when does life is strange before the storm part <end>\n",
            "Predicted answer: <start> october <end> \n",
            "\n",
            "Question: <start> where is sodom and gomorrah located in the bible <end>\n",
            "Predicted answer: <start> on the southern region of the land of canaan <end> \n",
            "The answer was: <start> on the jordan river plain in the southern region of the land of canaan <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> what is the longest non medical word in the dictionary <end>\n",
            "Predicted answer: <start> louise liberty williams <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> when did the xbox slim come out <end>\n",
            "Predicted answer: <start> in <end> \n",
            "\n",
            "Question: <start> what time of year do ravens lay eggs <end>\n",
            "Predicted answer: <start> the st andrews links trust <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who spread the theory that one is a product of the mind and body <end>\n",
            "Predicted answer: <start> rene descartes <end> \n",
            "\n",
            "Question: <start> how many goals scored ronaldo in his career <end>\n",
            "Predicted answer: <start> september , <end> \n",
            "The answer was: <start> over <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who played tess on touched by an angel <end>\n",
            "Predicted answer: <start> delloreese patricia early july , november , , known professionally as della reese <end> \n",
            "\n",
            "Question: <start> when did the hornets move to new orleans <end>\n",
            "Predicted answer: <start> season <end> \n",
            "\n",
            "Question: <start> fish appeared in the fossil record during the <end>\n",
            "Predicted answer: <start> the cambrian explosion <end> \n",
            "\n",
            "Question: <start> who died from the band faith no more <end>\n",
            "Predicted answer: <start> bastille day <end> \n",
            "The answer was: <start> singer chuck mosley <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> who signed the gun control act of <end>\n",
            "Predicted answer: <start> president lyndon b . johnson <end> \n",
            "\n",
            "Question: <start> where do frankenstein and the monster first meet <end>\n",
            "Predicted answer: <start> the mountains <end> \n",
            "\n",
            "Question: <start> the human tendency to mimic other people s behavior is an example of <end>\n",
            "Predicted answer: <start> mirroring <end> \n",
            "\n",
            "Question: <start> when does season five of the killing come out <end>\n",
            "Predicted answer: <start> march , <end> \n",
            "The answer was: <start> a fourth season consisting of six episodes to conclude the series <end>\n",
            "\n",
            "\n",
            "\n",
            "Question: <start> how is the head of the church of england <end>\n",
            "Predicted answer: <start> the monarch is the supreme governor <end> \n",
            "\n",
            "76 answers out of 101 possible, rate is 75%\n",
            "At least 52 correct answers out of 76 possible, rate is 68%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}